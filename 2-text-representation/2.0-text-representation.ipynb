{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Representation\n",
    "It refers to converting a text into a format that can be understood by computers. For Natural Language Processing (NLP) problems, transforming text into numerical data is a crucial step. Text representation techniques are used to convey the structure, meaning, and context of the text to the computer.\n",
    "\n",
    "![text-representation](../images/2/2-text-representation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Bag of Words (BoW):\n",
    "  Bag of Words (BoW) treats the words in a text like items in a \"bag,\" disregarding the order or grammatical structure of the words. It focuses only on the presence and frequency of words. This method treats each word in the text as a feature and counts the frequency of those words.\n",
    "\n",
    "  * How Does It Work?\n",
    "    * Each word in the text becomes a \"feature.\"\n",
    "     * The frequencies of these features allow the representation of each text as a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - #### Term Frequency-Inverse Document Frequency (TF-IDF):\n",
    "    TF-IDF is a metric used to determine how important a word is within a document. Term Frequency (TF) measures the frequency of the word in the document, while Inverse Document Frequency (IDF) measures how rare the word is across all documents. This allows common words (e.g., \"and\", \"the\") to be given lower weight, while rare and significant words are given higher weight.\n",
    "\n",
    "  * How It Works?\n",
    "    * TF: The number of times a word appears in a specific text.\n",
    "    * IDF: Calculated based on how widespread the word is across all documents. Rare words have a higher IDF value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### N-Gram: \n",
    "  N-Gram divides a text into n consecutive words (or characters). N-Grams are often used to understand the context between words better. This method takes into account the sequence of words.\n",
    "\n",
    "  * How It Works?\n",
    "    * Unigram: Represents a single word (n=1).\n",
    "    * Bigram: Represents two consecutive words (n=2).\n",
    "    * Trigram: Represents three consecutive words (n=3).\n",
    "\n",
    "  * For example, in the sentence \"The cat is on the mat\":\n",
    "    * Unigrams: [\"The\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]\n",
    "    * Bigrams: [\"The cat\", \"cat is\", \"is on\", \"on the\", \"the mat\"]\n",
    "    * Trigrams: [\"The cat is\", \"cat is on\", \"is on the\", \"on the mat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Word Embeddings: \n",
    "  Word embeddings represent words using high-dimensional dense vectors. The semantic relationships and context information between words are captured in these vectors. Word embeddings can learn semantic similarities between words, such as the relationship between \"king\" and \"queen.\"\n",
    "\n",
    "  * Popular Methods:\n",
    "    * Word2Vec: A model that learns the similarities between words. For example, it can represent relationships like \"King\" - \"Man\" + \"Woman\" = \"Queen.\"\n",
    "    * GloVe (Global Vectors for Word Representation): Another word embedding model focused on understanding word relationships and context.\n",
    "    * FastText: Provides more accurate representations by considering subwords.\n",
    "    \n",
    "  * How It Works:\n",
    "    * Each word is converted into a vector.\n",
    "    * Vectors are trained to learn meaningful relationships between words, capturing their context and semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Transformer-Based Text Representation (BERT, GPT, T5):\n",
    "  Transformer-based text representation dynamically represents words and sentences based on context. These models learn the meaning of each word in the text by considering the relationships between the word and other words in the sentence. The transformer architecture uses an attention mechanism to learn the context of each word relative to the others, allowing for more accurate and meaningful understanding of the text. These models are highly effective in solving a variety of NLP tasks.\n",
    "\n",
    "  * BERT (Bidirectional Encoder Representations from Transformers): \n",
    "  BERT learns the meaning of each word by considering both the preceding and succeeding words in the context. BERT is a bidirectional model, meaning it learns the meaning of a word by considering both the words to the left and the words to the right of it. This helps in understanding the language in a more comprehensive way. BERT is particularly successful in tasks like text understanding, question answering, and text classification.\n",
    "\n",
    "    * How It Works:\n",
    "      * Pre-training: BERT is pre-trained on a large text corpus to learn word contexts. During this phase, a technique called \"Masked Language Modeling\" (MLM) is used, where some words in the text are masked, and the model tries to predict those masked words.\n",
    "      * Fine-tuning: After pre-training, BERT is fine-tuned for specific tasks like classification or question answering.\n",
    "      \n",
    "    * Applications:\n",
    "      * Text classification, sentiment analysis\n",
    "      * Question answering\n",
    "      * Named Entity Recognition (NER)\n",
    "      * Sentence similarity analysis\n",
    "\n",
    "\n",
    "  * GPT (Generative Pre-trained Transformer):\n",
    "  GPT is highly effective for text generation tasks. After being trained on a large corpus of text, the model can generate continuations for a given starting sentence. GPT operates unidirectionally (only considering previous words) and excels at word or sentence generation.\n",
    "\n",
    "    * How It Works:\n",
    "      * Pre-training: GPT is trained to complete text by looking only at previous words. During training, the model is trained as a unidirectional language model.\n",
    "      * Fine-tuning: The model can be fine-tuned for specific tasks to produce more task-specific results.\n",
    "\n",
    "    * Applications:\n",
    "      * Text generation (story generation)\n",
    "      * Speech generation\n",
    "      * Chatbots and text-based dialogue systems\n",
    "\n",
    "\n",
    "  * T5 (Text-to-Text Transfer Transformer):\n",
    "  T5 stands out among transformer-based models for its versatility. It solves all NLP tasks in a \"text-to-text\" format, meaning every NLP task is viewed as converting one text into another. For instance, tasks like text summarization, translation, and question answering are all handled by transforming one piece of text into another. T5 combines aspects of both BERT and GPT by considering both directions in its processing.\n",
    "\n",
    "    * How It Works:\n",
    "      * Text-to-Text: T5 takes a text input and appends a task-specific prefix (e.g., \"summarize:\" or \"translate:\"). The model generates the appropriate output based on this prefix.\n",
    "      * T5 is trained on a wide variety of tasks during pre-training, making it suitable for any NLP task.\n",
    "\n",
    "    * Applications:\n",
    "      * Text summarization\n",
    "      * Question answering\n",
    "      * Translation\n",
    "      * Text classification\n",
    "      * Dialogue and text-based applications\n",
    "\n",
    "\n",
    "  * Key Differences Between Transformer Models:\n",
    "    * BERT: Bidirectional (context from both previous and following words) and focused on learning the meaning of text. Typically used for text understanding and classification.\n",
    "    * GPT: Unidirectional and optimized for text generation. It generates continuations based on the initial sentence.\n",
    "    * T5: Works in \"Text-to-Text\" format and uses a single structure to solve a wide variety of NLP tasks. Effective for both text generation and comprehension tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
