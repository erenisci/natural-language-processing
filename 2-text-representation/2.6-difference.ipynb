{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference Between Text Representation Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method                 | Key Features                                                                                                                            | Ease of Use                                                                                                                           | Performance                                                                                                     |\n",
    "| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |\n",
    "| **BOW (Bag of Words)** | - Based on word frequencies. <br> - Represents text as word counts. <br> - Does not capture context or relationships between words.     | - Simple and fast. <br> - Easy to use.                                                                                                | - Effective for small datasets. <br> - Low performance in capturing context and semantic relationships.         |\n",
    "| **n-gram**             | - Uses word sequences (n words) to represent text. <br> - Captures some relationships between connected words.                          | - Can be easily applied. <br> - May require parameter tuning (e.g., n value).                                                         | - Better at capturing some relationships between words. <br> - Very high n-grams can increase model complexity. |\n",
    "| **TF-IDF**             | - Considers both word frequency and rarity in the document. <br> - Highlights relevant and unique words.                                | - Easy to use and supported in most languages. <br> - Widely used in tasks like sentiment analysis, text classification.              | - Provides good representation, especially when important words matter. <br> - Limited in capturing context.    |\n",
    "| **Word Embeddings**    | - Represents words as continuous vectors. <br> - Captures semantic similarities. <br> - Learns relationships between words and context. | - May be harder to learn. <br> - May require pre-trained models or training.                                                          | - Excellent at capturing contextual relationships. <br> - High performance in semantic similarity.              |\n",
    "| **Transformers**       | - Based on deep learning. <br> - Uses unique attention mechanisms. <br> - Highly effective in understanding context.                    | - Most transformer models (BERT, GPT, etc.) are large and complex. <br> - Training can be time-consuming and requires high resources. | - Best performance in contextual understanding. <br> - Delivers top results in NLP tasks.                       |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
