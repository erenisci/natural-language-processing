{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)\n",
    "\n",
    "It is a simple and widely used technique for obtaining a numerical representation of text data. BoW creates a \"bag\" containing words or tokens but does not consider the order of the words or grammatical structures. In other words, it focuses solely on the frequency of the words when representing the meaning of the text.\n",
    "\n",
    "![\"bow\"](../images/2/2-bow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Words): \n",
      " ['also' 'and' 'are' 'but' 'cats' 'clever' 'cute' 'dogs' 'not' 'only'\n",
      " 'playful' 'very']\n",
      "BoW Array: \n",
      " [[0 1 2 0 1 0 1 1 0 0 1 2]\n",
      " [1 0 1 1 1 1 1 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents_example = [\n",
    "    \"Cats are very cute and dogs are very playful.\",\n",
    "    \"Cats are not only cute but also clever.\",\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Text -> Vector / -> [Vectorizer]\n",
    "X = vectorizer.fit_transform(documents_example)\n",
    "\n",
    "# Display the BoW representation and the corresponding words\n",
    "print(\"Feature Names (Words): \\n\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Array: \\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real-Life Application of BoW Using the IMDB Dataset\n",
    "\n",
    "- The dataset link &rarr; [IMDB_Dataset.csv](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iscie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "# Getting the data\n",
    "df = pd.read_csv(\"../data/IMDB_Dataset.csv\")\n",
    "documents = df[\"review\"]\n",
    "labels = df[\"sentiment\"]\n",
    "\n",
    "\n",
    "# Text Cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Punctuation\n",
    "\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Special characters\n",
    "\n",
    "    text = \" \".join(\n",
    "        [word for word in text.split() if word not in stop_words and len(word) > 2]\n",
    "    )  # Stopwords and [len(word) > 2 words]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean texts\n",
    "cleaned_documents = [clean_text(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Representation:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# BoW\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Text -> Vector\n",
    "X = vectorizer.fit_transform(cleaned_documents[:100])\n",
    "\n",
    "# Word set\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Vector Representation\n",
    "print(\"Vector Representation:\")\n",
    "print(X.toarray()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abbot  abbreviated  abetted  abiding  ability  able  aboveaverage  abraham  \\\n",
      "0      0            0        0        0        0     0             0        0   \n",
      "1      0            0        0        0        0     0             0        0   \n",
      "2      0            0        0        0        0     0             0        0   \n",
      "3      0            0        0        0        0     0             0        0   \n",
      "4      0            0        0        0        0     0             0        0   \n",
      "\n",
      "   abrahams  absolute  ...  zany  zellweger  zerog  zeus  zombie  zombiebr  \\\n",
      "0         0         0  ...     0          0      0     0       0         0   \n",
      "1         0         0  ...     0          0      0     0       0         0   \n",
      "2         0         0  ...     0          0      0     0       0         0   \n",
      "3         0         0  ...     0          0      0     0       1         1   \n",
      "4         0         0  ...     0          0      0     0       0         0   \n",
      "\n",
      "   zone  zoo  zooms  zwick  \n",
      "0     0    0      0      0  \n",
      "1     0    0      0      0  \n",
      "2     0    0      0      0  \n",
      "3     0    0      0      0  \n",
      "4     0    0      0      0  \n",
      "\n",
      "[5 rows x 4796 columns]\n"
     ]
    }
   ],
   "source": [
    "# DataFrame of Vector Representation\n",
    "df_bow = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "print(df_bow.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of Words\n",
    "word_counts = X.sum(axis=0).A1\n",
    "word_frequency = dict(zip(feature_names, word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', np.int64(169)), ('film', np.int64(127)), ('one', np.int64(100)), ('like', np.int64(80)), ('even', np.int64(58))]\n"
     ]
    }
   ],
   "source": [
    "# Common 5 word\n",
    "most_common = Counter(word_frequency).most_common(5)\n",
    "print(most_common)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
