{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Models\n",
    "\n",
    "N-gram Models are a technique used in natural language processing and language modeling that models sequences of words (or symbol sequences) of a specific length. These models learn the sequential dependencies between words and are used to predict future words. Essentially, the probability of a word or word sequence is computed based only on the preceding N words.\n",
    "\n",
    "![n-gram-models](../images/3/3-n-gram-models.png)\n",
    "\n",
    "Here, N refers to the length of the word sequence. For example:\n",
    "\n",
    "- 1-gram (Unigram): Each word is considered independently.\n",
    "- 2-gram (Bigram): The probability of a word depends only on the previous word.\n",
    "- 3-gram (Trigram): The probability of a word depends on the previous two words.\n",
    "\n",
    "N-gram models typically learn by counting the frequency of each word sequence and then calculating probabilities based on these frequencies.\n",
    "\n",
    "---\n",
    "\n",
    "#### Use Cases of N-gram Models\n",
    "\n",
    "- Language Modeling: N-gram models are used to learn the structure of a language and generate text that follows grammatical rules. For instance, completing a sentence or predicting a sequence of words.\n",
    "- Spell Checking: Used to correct spelling mistakes or provide suggestions. For example, predicting the correct spelling of a misspelled word.\n",
    "- Machine Translation: N-gram models can be used to translate words from the source language to the target language. They are particularly useful for matching sequential words accurately.\n",
    "- Text Classification: N-grams can be used as features in text classification tasks, such as spam email detection or sentiment analysis.\n",
    "- Speech Recognition: N-gram models can be used to predict the correct words while transcribing spoken commands into text.\n",
    "- Information Retrieval (Search Engines): N-gram models can be used to predict the most likely results related to search queries.\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Simplicity: N-gram models have a very simple structure and are easy to set up. They learn word dependencies by only considering previous words.\n",
    "- Efficient Computation: Learning and prediction processes are fast because these models only consider fixed-length word sequences and typically require remembering only a few previous words.\n",
    "- Effective Language Modeling: Especially useful for smaller datasets, as they are good for understanding and predicting the structure of language. N-grams can be effective in learning sequential relationships.\n",
    "- Easy Implementation: N-grams can be practically applied in nearly every NLP task, making them a common solution.\n",
    "\n",
    "#### Disadvantages\n",
    "\n",
    "- Dependency Issues: N-gram models only consider the previous N-1 words, which ignores dependencies over longer distances. This can be problematic, especially in long sentences or texts with more complex linguistic structures.\n",
    "- Data Sparsity: If your N-gram model is large (e.g., trigrams or four-grams), you'll need a very large dataset to compute all the probabilities. This can make it difficult to predict rare word sequences and lead to data sparsity issues.\n",
    "- Large Data Requirements: Larger N values require more data. For example, trigram or four-gram models require very large datasets, increasing computational costs.\n",
    "- Complexity: As you create larger language models, higher-order N-grams can consume more memory and take longer to process.\n",
    "- General Performance Limitations: N-gram models may struggle to capture deep context and semantic relationships in language. This is why more complex models (e.g., deep learning-based models) often provide better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "corpus = [\n",
    "    \"I love you\",\n",
    "    \"I love apple\",\n",
    "    \"I love programming\",\n",
    "    \"You love me\",\n",
    "    \"She loves apple\",\n",
    "    \"They love you\",\n",
    "    \"I love you and you love me\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'love', 'you'], ['i', 'love', 'apple'], ['i', 'love', 'programming'], ['you', 'love', 'me'], ['she', 'loves', 'apple'], ['they', 'love', 'you'], ['i', 'love', 'you', 'and', 'you', 'love', 'me']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "tokens = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'love'), ('love', 'you'), ('i', 'love'), ('love', 'apple'), ('i', 'love'), ('love', 'programming'), ('you', 'love'), ('love', 'me'), ('she', 'loves'), ('loves', 'apple'), ('they', 'love'), ('love', 'you'), ('i', 'love'), ('love', 'you'), ('you', 'and'), ('and', 'you'), ('you', 'love'), ('love', 'me')]\n"
     ]
    }
   ],
   "source": [
    "# n-gram -> n=2\n",
    "bigrams = []\n",
    "for token_list in tokens:\n",
    "    bigrams.extend(ngrams(token_list, 2))\n",
    "\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('i', 'love'): 4, ('love', 'you'): 3, ('you', 'love'): 2, ('love', 'me'): 2, ('love', 'apple'): 1, ('love', 'programming'): 1, ('she', 'loves'): 1, ('loves', 'apple'): 1, ('they', 'love'): 1, ('you', 'and'): 1, ('and', 'you'): 1})\n"
     ]
    }
   ],
   "source": [
    "# Bigram Frequency Counter\n",
    "bigrams_freq = Counter(bigrams)\n",
    "\n",
    "print(bigrams_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'love', 'you'), ('i', 'love', 'apple'), ('i', 'love', 'programming'), ('you', 'love', 'me'), ('she', 'loves', 'apple'), ('they', 'love', 'you'), ('i', 'love', 'you'), ('love', 'you', 'and'), ('you', 'and', 'you'), ('and', 'you', 'love'), ('you', 'love', 'me')]\n"
     ]
    }
   ],
   "source": [
    "# n-gram -> n=3\n",
    "trigrams = []\n",
    "for token_list in tokens:\n",
    "    trigrams.extend(ngrams(token_list, 3))\n",
    "\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('i', 'love', 'you'): 2, ('you', 'love', 'me'): 2, ('i', 'love', 'apple'): 1, ('i', 'love', 'programming'): 1, ('she', 'loves', 'apple'): 1, ('they', 'love', 'you'): 1, ('love', 'you', 'and'): 1, ('you', 'and', 'you'): 1, ('and', 'you', 'love'): 1})\n"
     ]
    }
   ],
   "source": [
    "# Bigram Frequency Counter\n",
    "trigrams_freq = Counter(trigrams)\n",
    "\n",
    "print(trigrams_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of the bigram \"I love\" being followed by \"you\" or \"apple\"\n",
    "i_love = (\"i\", \"love\")  # (\"i\", \"love\")\n",
    "\n",
    "prob_you = trigrams_freq[(i_love + (\"you\",))] / bigrams_freq[i_love]\n",
    "prob_apple = trigrams_freq[(i_love + (\"apple\",))] / bigrams_freq[i_love]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of you: 0.5\n",
      "Probability of apple: 0.25\n"
     ]
    }
   ],
   "source": [
    "print(\"Probability of you:\", prob_you)\n",
    "print(\"Probability of apple:\", prob_apple)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
