{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Models (HMM)\n",
    "\n",
    "A Hidden Markov Model (HMM) is a probabilistic model where a system has unobservable (hidden) states that generate observable outputs. A Markov Chain is a process where the future state of a system depends only on the current state and is independent of previous states. HMM extends this concept: the real states (hidden states) cannot be directly observed, but we can make predictions using the observable outputs that depend on them.\n",
    "\n",
    "![hidden-markov-models](../images/3/3-hidden-markov-models.png)\n",
    "\n",
    "- #### How Does HMM Work?\n",
    "\n",
    "  An HMM consists of five key components:\n",
    "\n",
    "  - Hidden States (S): The underlying states that cannot be directly observed (e.g., words or phonemes in speech recognition).\n",
    "  - Observable States (O): The output sequences produced by the hidden states (e.g., spectral features of speech sounds).\n",
    "  - Initial Probabilities (π): The probability of starting in a particular hidden state.\n",
    "  - Transition Probabilities (A): The probabilities of transitioning from one hidden state to another.\n",
    "  - Observation Probabilities (B): The probabilities of a hidden state generating a specific observable output.\n",
    "    <br>\n",
    "    <br>\n",
    "\n",
    "---\n",
    "\n",
    "- #### Applications of HMM\n",
    "- Speech Recognition:\n",
    "  - Used for converting spoken words into text.\n",
    "  - Example: Extracting words from speech waveforms.\n",
    "- Natural Language Processing (NLP):\n",
    "  - Part-of-Speech Tagging: Identifying whether a word in a sentence is a noun, verb, adjective, etc.\n",
    "  - Named Entity Recognition (NER): Detecting names of people, locations, and organizations in a text.\n",
    "  - Spelling Correction and Language Modeling: Evaluating the probability of a word or sentence being meaningful.\n",
    "- Bioinformatics:\n",
    "  - Gene Sequencing (DNA Sequence Analysis): Predicting genes and motifs in DNA sequences.\n",
    "  - Protein Structure Prediction: Identifying whether a protein’s amino acid sequence corresponds to a specific structure.\n",
    "- Finance and Economics:\n",
    "  - Market Analysis: Used for modeling stock price movements.\n",
    "  - Risk Analysis: Modeling transitions between different hidden states of financial markets.\n",
    "- Robotics and Image Processing:\n",
    "\n",
    "  - Motion Recognition: Identifying human movements (walking, running, jumping) in video analysis.\n",
    "  - Optical Character Recognition (OCR): Recognizing handwritten or printed text using HMM techniques.\n",
    "\n",
    "- #### Advantages of HMM\n",
    "\n",
    "  - Simple yet Powerful Probabilistic Model:\n",
    "    HMM effectively models transitions between states using probability-based reasoning.\n",
    "  - Can Handle Missing Data:\n",
    "    Since it models hidden states, HMM is useful for dealing with missing or incomplete data.\n",
    "  - Suitable for Real-World Sequential Processes:\n",
    "    HMM is ideal for analyzing time-dependent sequences like speech, language, and genetic sequences.\n",
    "  - Can Be Combined with Machine Learning:\n",
    "    HMM can be integrated with deep learning (e.g., LSTMs) to build more powerful models.\n",
    "\n",
    "- #### Disadvantages of HMM\n",
    "  - Limited Dependency Modeling (Markov Assumption)\n",
    "    Assumes that the current state depends only on the previous state, which may not be sufficient for some problems.\n",
    "  - Difficult Learning Process (High Training Cost)\n",
    "    Training HMMs often requires the Baum-Welch Algorithm (a variant of the Expectation-Maximization (EM) Algorithm) or the Viterbi Algorithm, which can be computationally expensive for large datasets.\n",
    "  - Weakness in Capturing Long-Range Dependencies\n",
    "    HMM struggles to model long-term dependencies in sequences, making Transformers and RNNs/LSTMs more suitable for modern NLP tasks.\n",
    "  - Lack of Exact Predictions\n",
    "    Since HMM is a probabilistic model, it does not provide exact results but only the most probable outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset\n",
    "train_data = [\n",
    "    [(\"I\", \"PRP\"), (\"am\", \"VBP\"), (\"a\", \"DT\"), (\"student\", \"NN\")],\n",
    "    [(\"You\", \"PRP\"), (\"are\", \"VBP\"), (\"a\", \"DT\"), (\"teacher\", \"NN\")],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM Training\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = trainer.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged sentence: [('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('teacher', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# New Sentence\n",
    "test_sentence = \"I am a teacher\".split()\n",
    "tags = hmm_tagger.tag(test_sentence)\n",
    "\n",
    "print(\"Tagged sentence:\", tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real-Life Application of Word Embeddings Using the IMDB Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.tag import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\iscie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the dataset\n",
    "nltk.download(\"conll2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build\n",
    "train_data = conll2000.tagged_sents(\"train.txt\")\n",
    "test_data = conll2000.tagged_sents(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM training\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = trainer.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('not', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('park', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_sentence = \"I am not going to park\".split()\n",
    "tags = hmm_tagger.tag(test_sentence)\n",
    "\n",
    "print(tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
