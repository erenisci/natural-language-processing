{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy (MaxEnt) Models\n",
    "\n",
    "The Maximum Entropy Model (MaxEnt) is a probabilistic classification model. It shares similarities with Logistic Regression but operates within a more general framework. Rooted in information theory, MaxEnt follows the principle of making the fewest possible assumptions while making probabilistic predictions.\n",
    "\n",
    "- Core Idea:\n",
    "\n",
    "  - When estimating the probabilities of an event, no unnecessary assumptions should be made.\n",
    "  - The goal is to determine the most suitable probability distribution for the observed data while maximizing entropy without introducing extra constraints.\n",
    "\n",
    "  ![\"maximum-entropy-models\"](../images/3/3-maximum-entropy-models.png)\n",
    "\n",
    "---\n",
    "\n",
    "- #### How It Works\n",
    "\n",
    "  The MaxEnt model learns a probability distribution based on given features.\n",
    "  The probability of an input belonging to a certain class is computed using an exponential function:\n",
    "\n",
    "  $$ P(y \\mid x) = \\frac{1}{Z(x)} \\exp \\left( \\sum\\_{i} \\lambda_i f_i (x, y) \\right) $$\n",
    "\n",
    "  Where:\n",
    "\n",
    "  - \\( x \\) → Input (feature set)\n",
    "  - \\( y \\) → Output (predicted class)\n",
    "  - \\( f_i(x, y) \\) → Feature functions\n",
    "  - \\( \\lambda_i \\) → Weights (learned by the model)\n",
    "  - \\( Z(x) \\) → Normalization constant\n",
    "    <br>\n",
    "    <br>\n",
    "\n",
    "- #### Summary:\n",
    "\n",
    "  - The model learns a probability distribution that maximizes entropy.\n",
    "  - Weights (\\( \\lambda_i \\)) are adjusted to best fit the training data.\n",
    "  - It does not impose extra assumptions, making it a flexible model.\n",
    "    <br>\n",
    "    <br>\n",
    "\n",
    "- #### Applications\n",
    "\n",
    "  - Natural Language Processing (NLP)\n",
    "    - Part-of-Speech Tagging (POS Tagging) → Assigning grammatical tags to words.\n",
    "    - Named Entity Recognition (NER) → Identifying entities (person, location, organization) in text.\n",
    "    - Word Sense Disambiguation → Determining the correct meaning of a word based on context.\n",
    "  - Speech Recognition\n",
    "    - Used to classify speech signals.\n",
    "    - Often combined with HMM models to improve accuracy.\n",
    "  - Computer Vision\n",
    "    - Applied in object recognition and image segmentation.\n",
    "  - Machine Learning and Data Mining\n",
    "    - Can be used for classification problems where features need to be assigned to categories.\n",
    "      <br>\n",
    "      <br>\n",
    "\n",
    "- #### Advantages\n",
    "\n",
    "  - Flexible and Assumption-Free\n",
    "\n",
    "    - The model does not make unnecessary assumptions and relies entirely on observed data.\n",
    "\n",
    "  - Feature Engineering-Friendly\n",
    "\n",
    "    - Since feature functions \\( f_i(x, y) \\) can be explicitly defined, custom features can be added for specific problems.\n",
    "\n",
    "  - Generalized Version of Logistic Regression\n",
    "\n",
    "    - If feature functions depend only on the input, the model is equivalent to logistic regression.\n",
    "    - However, it can go beyond logistic regression for more complex problems.\n",
    "\n",
    "  - Effective for NLP Tasks - Can outperform HMM and Naive Bayes in NLP applications.\n",
    "    <br>\n",
    "    <br>\n",
    "\n",
    "- #### Disadvantages\n",
    "- High Computational Cost\n",
    "\n",
    "  - Requires iterative optimization techniques (such as Gradient Descent, L-BFGS).\n",
    "  - Training can be slow on large datasets.\n",
    "\n",
    "- Risk of Overfitting\n",
    "  - Adding too many features may cause the model to overfit the training data.\n",
    "- Limited Ability to Model Dependencies\n",
    "  - Not designed for modeling sequential dependencies like HMM.\n",
    "  - For time-series or sequential predictions, models like HMM or RNN/LSTM may be better alternatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import MaxentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    ({\"love\": True, \"amazing\": True}, \"positive\"),\n",
    "    ({\"hate\": True, \"terrible\": True}, \"negative\"),\n",
    "    ({\"happy\": True, \"joy\": True}, \"positive\"),\n",
    "    ({\"sad\": True, \"depressed\": True}, \"negative\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.500\n",
      "             2          -0.40547        1.000\n",
      "             3          -0.28768        1.000\n",
      "             4          -0.22314        1.000\n",
      "             5          -0.18232        1.000\n",
      "             6          -0.15415        1.000\n",
      "             7          -0.13353        1.000\n",
      "             8          -0.11778        1.000\n",
      "             9          -0.10536        1.000\n",
      "         Final          -0.09531        1.000\n"
     ]
    }
   ],
   "source": [
    "# Max Ent Classifier Training\n",
    "classifier = MaxentClassifier.train(train_data, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': False, 'amazing': False, 'hate': True, 'terrible': False, 'happy': False}\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_sentence = \"I hate this bad movie\"\n",
    "features = {\n",
    "    word: (word in test_sentence.lower().split())\n",
    "    for word in [\"love\", \"amazing\", \"hate\", \"terrible\", \"happy\"]\n",
    "}\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "label = classifier.classify(features)\n",
    "print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
