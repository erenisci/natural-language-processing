{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Language Models\n",
    "\n",
    "Probabilistic Language Models are systems that attempt to model the structure and meaning of language through probabilities. These models predict the probabilities of words or word sequences and make predictions by using data to learn the characteristics of language. Essentially, they try to model the uncertainty within language by discovering its structural relationships.\n",
    "\n",
    "![probabilistic-language-models](../images/3/3-probabilistic-language-models.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram Models\n",
    "\n",
    "- N-gram is one of the most basic approaches used in probabilistic modeling of language. In this model, words or groups of words are grouped into specific lengths (e.g., 1-gram, 2-gram, 3-gram, etc.).\n",
    "\n",
    "- Fundamentals of N-gram Model: In this model, the probability of a word or word sequence depends only on the previous N words. Example: In a Bigram (2-gram) model, the probability of a word depends only on the preceding word. For example, the probability of the word \"yaz\" (writing) coming after \"g√ºzel\" (beautiful) is predicted. Use Cases: Text generation, language modeling, and automatic spell checking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Markov Models (HMM)\n",
    "\n",
    "- Hidden Markov Models are probabilistic models used primarily when working with sequential data. This model assumes that observed data (words, sounds, etc.) are based on certain hidden states.\n",
    "\n",
    "- HMM learns the probability of each observation (e.g., a word) belonging to a specific hidden state (e.g., grammatical categories). The model calculates the probability of each observation while transitioning between hidden states. Example: In tasks like part-of-speech tagging or speech recognition, HMM is used to predict which label a word is associated with. For example, predicting whether a word is a verb or a noun. Use Cases: Commonly used in Natural Language Processing (NLP) and speech recognition applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Entropy Models\n",
    "\n",
    "- Maximum entropy models aim to create a probability distribution that maximizes entropy, based on limited information obtained from the data. Here, the model creates the most \"uniform\" distribution, making the least assumptions.\n",
    "\n",
    "- The maximum entropy model learns a probability distribution that has the highest entropy based on the accuracy of the given features. It makes the least number of assumptions about the unknowns in the data. Example: In natural language processing tasks such as text classification or tagging, it can be used to predict the probability of a word or word sequence belonging to a certain class. Use Cases: Commonly used in NLP tasks like classification and tagging, especially for making probability predictions with minimal assumptions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
