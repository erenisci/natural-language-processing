{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval (w/ BERT)\n",
    "\n",
    "Information Retrieval is the process of obtaining relevant information from large datasets, typically documents or databases, in response to a query. The main goal of an IR system is to find and retrieve the most relevant data based on the userâ€™s needs.\n",
    "\n",
    "#### Key Concepts in Information Retrieval\n",
    "\n",
    "1. **Document**: A unit of information that is stored in the system, such as a web page, article, or any piece of data.\n",
    "2. **Query**: The search term or question provided by the user to retrieve relevant documents.\n",
    "3. **Ranking**: The process of ordering retrieved documents based on their relevance to the query.\n",
    "4. **Relevance**: A measure of how well a document satisfies the user's query.\n",
    "\n",
    "#### Types of Information Retrieval\n",
    "\n",
    "1. **Boolean Retrieval**: This model retrieves documents based on exact matches to the query terms. The query is typically represented using Boolean operators (AND, OR, NOT).\n",
    "\n",
    "   - Example: A query \"apple AND orange\" retrieves documents that contain both \"apple\" and \"orange.\"\n",
    "\n",
    "2. **Vector Space Model (VSM)**: In VSM, documents and queries are represented as vectors in a multi-dimensional space. The relevance is determined by the cosine similarity between the query and the document vectors.\n",
    "\n",
    "   - Example: A query \"apple fruit\" is compared to document vectors, and the closest matching document is retrieved.\n",
    "\n",
    "3. **Probabilistic Retrieval**: Probabilistic models estimate the likelihood that a document is relevant to a given query. One of the well-known models is the BM25 (Best Matching 25), which ranks documents based on term frequency, inverse document frequency, and document length.\n",
    "\n",
    "#### Information Retrieval Example\n",
    "\n",
    "- **Query**: \"Best smartphones 2025\"\n",
    "- **Documents**: Articles, blogs, and product reviews about smartphones for the year 2025.\n",
    "- **Retrieved Documents**: The system retrieves documents that provide relevant information about the best smartphones of 2025.\n",
    "\n",
    "#### Challenges in Information Retrieval\n",
    "\n",
    "- **Ambiguity**: Queries may have multiple meanings or interpretations, which makes it hard to match documents.\n",
    "- **Relevance Ranking**: Determining the most relevant documents among a large collection can be complex.\n",
    "- **Handling Noise**: Some irrelevant or low-quality documents may be retrieved along with relevant ones, requiring better filtering techniques.\n",
    "\n",
    "#### Modern Approaches in IR\n",
    "\n",
    "- **Search Engines**: Modern search engines like Google, Bing, and others are built on sophisticated IR models that incorporate machine learning, natural language processing, and deep learning techniques to rank and retrieve documents more accurately.\n",
    "- **Neural IR**: Deep learning models, such as those based on BERT or T5, have been applied to IR tasks to improve the relevance ranking of search results by understanding the context of queries and documents better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: 0.7235309481620789\n",
      "Document 2: 0.6327658891677856\n",
      "Document 3: 0.698382556438446\n",
      "Document 4: 0.6562472581863403\n",
      "\n",
      "Most similar document\n",
      "Machine learning is a field of artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# BERT Model and Tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Documents and query\n",
    "documents = [\n",
    "    \"Machine learning is a field of artificial intelligence.\",\n",
    "    \"Banking system involves understanding human language.\",\n",
    "    \"Calculus encompasses probability\",\n",
    "    \"Data science combines statistics, data analysis, and machine learning.\",\n",
    "]\n",
    "query = \"What is machine learning?\"\n",
    "\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Extract the last hidden state from the model output\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    embedding = last_hidden_state.mean(dim=1)\n",
    "\n",
    "    return embedding.detach().numpy()\n",
    "\n",
    "\n",
    "# Get embedding vectors for documents and query\n",
    "doc_embeddings = np.vstack([get_embedding(doc) for doc in documents])\n",
    "query_embedding = get_embedding(query)\n",
    "\n",
    "# Cosine similarity calculation\n",
    "similarities = cosine_similarity(query_embedding, doc_embeddings)\n",
    "\n",
    "# Print the similarity scores for each document\n",
    "for i, score in enumerate(similarities[0]):\n",
    "    print(f\"Document {i+1}: {score}\")\n",
    "\n",
    "# Get the most similar document\n",
    "most_similar_index = similarities.argmax()\n",
    "print(\"\\nMost similar document\")\n",
    "print(documents[most_similar_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
