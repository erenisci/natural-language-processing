{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering (QA) (w/ BERT - GPT)\n",
    "\n",
    "Question Answering (QA) is a task in Natural Language Processing (NLP) that involves developing models capable of understanding a question and providing an appropriate answer based on a given context. This task can be approached in several ways, including extracting answers from documents or generating answers based on pre-existing knowledge.\n",
    "\n",
    "#### Types of Question Answering\n",
    "\n",
    "1. **Extractive QA**: In extractive question answering, the system identifies and selects a portion of the input text that contains the correct answer. For instance, given a passage, the model extracts a span of text that directly answers the question. An example:\n",
    "\n",
    "   - Question: \"Who wrote 'Pride and Prejudice'?\"\n",
    "   - Context: \"Jane Austen wrote 'Pride and Prejudice' in 1813.\"\n",
    "   - Answer: \"Jane Austen\"\n",
    "\n",
    "2. **Abstractive QA**: Abstractive question answering generates a summary or a rephrased version of the information to form a natural language answer. This approach goes beyond extracting text directly from the document and involves generating new sentences to convey the answer. For instance:\n",
    "   - Question: \"What is the capital of France?\"\n",
    "   - Context: \"France is a country located in Europe, and its capital is Paris.\"\n",
    "   - Answer: \"Paris is the capital of France.\"\n",
    "\n",
    "#### Key Components of QA Systems\n",
    "\n",
    "- **Context**: The document, passage, or knowledge base from which the answer will be derived.\n",
    "- **Question**: The user's query that requires an answer.\n",
    "- **Answer**: The output of the QA model, which can either be a direct text extraction or a generated response.\n",
    "\n",
    "#### Example of QA\n",
    "\n",
    "- **Context**: \"Albert Einstein was a theoretical physicist, widely recognized for developing the theory of relativity.\"\n",
    "- **Question**: \"What is Albert Einstein known for?\"\n",
    "- **Answer**: \"Developing the theory of relativity.\"\n",
    "\n",
    "#### Challenges in QA\n",
    "\n",
    "- **Ambiguity**: Questions may have multiple possible interpretations.\n",
    "- **Complexity**: Long and complex questions may require advanced reasoning to extract or generate accurate answers.\n",
    "- **Context Understanding**: Models need to understand the context thoroughly to provide the most relevant answer.\n",
    "\n",
    "In recent advancements, transformer-based models such as BERT, T5, and GPT have been used for QA, where fine-tuning these pre-trained models on QA datasets has led to state-of-the-art performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "# BERT model fine-tuned on the SQuAD dataset for Question Answering\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def predict_answer(question, context):\n",
    "    # Tokenize the question and context\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        question, context, return_tensors=\"pt\", max_length=512, truncation=True\n",
    "    )\n",
    "\n",
    "    # Input tensors\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "    # Run the model and get the start and end scores\n",
    "    with torch.no_grad():\n",
    "        start_scores, end_scores = model(\n",
    "            input_ids, attention_mask=attention_mask, return_dict=False\n",
    "        )\n",
    "\n",
    "    # Find the highest probability start and end indices\n",
    "    start_index = torch.argmax(start_scores, dim=1).item()\n",
    "    end_index = torch.argmax(end_scores, dim=1).item()\n",
    "\n",
    "    # Get the tokens and decode the answer\n",
    "    answer_tokens = tokenizer.convert_ids_to_tokens(\n",
    "        input_ids[0][start_index : end_index + 1]\n",
    "    )\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n",
      "paris\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "context = \"France, officially the French Republic, is a country whose capital is Paris.\"\n",
    "answer = predict_answer(question=question, context=context)\n",
    "\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    # Format the input text by combining the question and context\n",
    "    input_text = f\"Question: {question} Context: {context} Please answer the Question according to Context.\"\n",
    "\n",
    "    # Tokenize the input text and convert it to tensors\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Create attention mask (consider all tokens)\n",
    "    attention_mask = torch.ones(inputs.shape, device=inputs.device)\n",
    "\n",
    "    # Run the model and generate the output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=256,  # Maximum length of the generated output\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,  # Use EOS token as padding\n",
    "            no_repeat_ngram_size=2,  # Prevent repetition of the same token combinations\n",
    "        )\n",
    "\n",
    "    # Decode the generated answer, skipping special tokens like EOS or padding\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the answer after the \"Answer:\" keyword\n",
    "    answer = answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is CPU?\n",
      "CPU is a term used to describe a processor that is used for a specific purpose. For example, a CPU can be used as a \"computer\" for \"programming\", or as an \"office\" or \"workstation\".\n",
      ". The term \"CPU\" is derived from the Latin word \"cpu\", which means \"processor\". The word is also used in the sense of \"processor\" and \"system\". In the context of computer programming, the term is often used interchangeably with \"machine\". For more information on the use of CPU, see the CPU FAQ.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is CPU?\"\n",
    "context = \"A CPU (Central Processing Unit) is the primary component of a computer that performs most of the processing inside. It interprets and executes instructions from programs, making it essential for the operation of a computer.\"\n",
    "answer = generate_answer(question=question, context=context)\n",
    "\n",
    "print(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
