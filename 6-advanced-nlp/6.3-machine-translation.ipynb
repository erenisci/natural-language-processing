{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Translation (w/ Transformers)\n",
    "\n",
    "Machine Translation (MT) is a subfield of Natural Language Processing (NLP) that focuses on automatically translating text or speech from one language to another. The primary goal of MT is to help bridge communication barriers between speakers of different languages by providing automated translation.\n",
    "\n",
    "![\"machine-translation\"](../images/6/6-machine-translation.png)\n",
    "\n",
    "#### Types of Machine Translation\n",
    "\n",
    "1. **Rule-Based Machine Translation (RBMT)**: This approach uses predefined linguistic rules and dictionaries to translate text. It involves a combination of syntax, grammar, and lexicons specific to the source and target languages.\n",
    "\n",
    "   - Example: \"I am learning NLP\" in English is translated to its equivalent structure in French using a set of language-specific rules.\n",
    "\n",
    "2. **Statistical Machine Translation (SMT)**: SMT relies on statistical models to translate text. It uses large bilingual corpora to learn translation probabilities between words and phrases. It involves two main stages: training the model on parallel corpora and using it to generate translations.\n",
    "\n",
    "   - Example: If the system has learned that \"dog\" frequently corresponds to \"chien\" in French, it will use this association for translation.\n",
    "\n",
    "3. **Neural Machine Translation (NMT)**: NMT uses deep learning, particularly neural networks, to translate text. It involves end-to-end models that learn the mapping between source and target languages in a more context-aware manner. NMT has shown significant improvements over traditional SMT models in terms of fluency and accuracy.\n",
    "\n",
    "   - Example: An NMT model learns from vast amounts of data and can generate translations like \"I am learning NLP\" as \"J'apprends le NLP\" in French with high fluency.\n",
    "\n",
    "4. **Hybrid Machine Translation**: Hybrid MT combines elements from multiple MT approaches (e.g., combining rule-based and statistical models) to leverage the strengths of each method and improve translation quality.\n",
    "\n",
    "#### Example of Machine Translation\n",
    "\n",
    "- **Source Sentence**: \"The cat is on the table.\"\n",
    "- **Translation (French)**: \"Le chat est sur la table.\"\n",
    "- **Translation (Spanish)**: \"El gato está en la mesa.\"\n",
    "\n",
    "#### Challenges in Machine Translation\n",
    "\n",
    "- **Ambiguity**: Words or phrases may have multiple meanings depending on context, making accurate translation difficult.\n",
    "  - Example: \"bank\" could mean a financial institution or the side of a river.\n",
    "- **Syntax and Grammar Differences**: Different languages have different sentence structures, which can make direct translation challenging. For example, word order may vary between languages.\n",
    "- **Cultural and Contextual Understanding**: Some phrases or idiomatic expressions may not have direct equivalents in the target language, requiring a deeper understanding of cultural context to provide a meaningful translation.\n",
    "- **Low-Resource Languages**: Many languages do not have enough available data or resources (e.g., parallel corpora) to train high-quality machine translation models.\n",
    "\n",
    "#### Modern Approaches in Machine Translation\n",
    "\n",
    "- **Transformer Models**: The introduction of transformer models, such as Google’s Transformer and OpenAI’s GPT, has revolutionized MT. These models use self-attention mechanisms to better capture the relationships between words across the entire sentence, leading to more accurate translations.\n",
    "- **Pretrained Language Models**: Models like BERT, GPT, and T5 have been fine-tuned for translation tasks and have improved machine translation systems by offering better contextual understanding and more fluent translations.\n",
    "\n",
    "- **Multilingual Models**: Modern MT systems often use a single model that can handle multiple languages. These multilingual models are trained on data from multiple language pairs and can translate between any of the supported languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, quel est votre nom?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-fr\"  # eng to fr\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "text = \"Hello, what is your name?\"\n",
    "\n",
    "# Encode\n",
    "translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n",
    "\n",
    "# Translate and --> String\n",
    "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
