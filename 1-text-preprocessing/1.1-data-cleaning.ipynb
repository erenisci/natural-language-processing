{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "The cleaning phase aims to remove unnecessary elements or those that do not help the model extract meaning, resulting in cleaner and more consistent data. This step reduces noise in the text, making the NLP process more effective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lowercasing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is lowercase!\n"
     ]
    }
   ],
   "source": [
    "uppercase_string = \"ThIs iS LowErCasE!\"\n",
    "lowercase_string = uppercase_string.lower()\n",
    "print(lowercase_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing Special Characters and Numbers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special characters 123456\n",
      "special characters \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# for special characters\n",
    "special_text = \"special, characters! 123456\"\n",
    "non_special_text = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", special_text)\n",
    "print(non_special_text)\n",
    "\n",
    "# for numbers\n",
    "number_text = \"special, characters! 123456\"\n",
    "non_number_text = re.sub(r\"[^A-Za-z\\s]\", \"\", special_text)\n",
    "print(non_number_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing Stop Words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'removing', 'stop', 'words', 'text', 'document.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = [\n",
    "    \"and\",\n",
    "    \"the\",\n",
    "    \"is\",\n",
    "    \"in\",\n",
    "    \"to\",\n",
    "    \"of\",\n",
    "    \"it\",\n",
    "    \"that\",\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"this\",\n",
    "    \"from\",\n",
    "]\n",
    "\n",
    "\n",
    "stopword_text = \"This is an example of removing stop words from a text document.\"\n",
    "\n",
    "\n",
    "filtered_words = [\n",
    "    word for word in stopword_text.split() if word.lower() not in stop_words\n",
    "]\n",
    "\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'removing', 'stop', 'words', 'text', 'document.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iscie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# stop word list\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "stopword_text = \"This is an example of removing stop words from a text document.\"\n",
    "filtered_words = [\n",
    "    word for word in stopword_text.split() if word.lower() not in stop_words\n",
    "]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bir', 'örnek', 'örneği', 'denemek', 'çalıştırıldı.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = [\"ve\", \"bu\", \"ile\", \"için\", \"ama\"]\n",
    "\n",
    "stopword_text = \"Bu bir örnek ve örneği denemek için çalıştırıldı.\"\n",
    "filtered_words = [\n",
    "    word for word in stopword_text.split() if word.lower() not in stop_words\n",
    "]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bir', 'örnek', 'örneği', 'denemek', 'çalıştırıldı.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iscie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# stop word list\n",
    "stop_words = set(stopwords.words(\"turkish\"))\n",
    "\n",
    "stopword_text = \"Bu bir örnek ve örneği denemek için çalıştırıldı.\"\n",
    "filtered_words = [\n",
    "    word for word in stopword_text.split() if word.lower() not in stop_words\n",
    "]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whitespace Removal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace is bad\n"
     ]
    }
   ],
   "source": [
    "whitespaced_text = \"    Whitespace    is       bad       \"\n",
    "non_whitespaced_text = \" \".join(whitespaced_text.split())\n",
    "print(non_whitespaced_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Punctuation Removal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuated text\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuated_text = \".punctuated, text!!!!.\"\n",
    "non_punctuated_text = punctuated_text.translate(\n",
    "    str.maketrans(\"\", \"\", string.punctuation)\n",
    ")\n",
    "print(non_punctuated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spelling Correction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct text\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "non_correct_text = \"corrrest text\"\n",
    "correct_text = str(TextBlob(non_correct_text).correct())\n",
    "print(correct_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing HTML Tags and URLs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to link\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_url_text = \"<a href='www.nlptest.com'>Go to link</a>\"\n",
    "non_html_url_text = BeautifulSoup(html_url_text, \"html.parser\").get_text()\n",
    "print(non_html_url_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All in one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before steps                            : <p>ThIs iS LowErCasE! special, characters! 123456 This is an example of removing stop words from a text document.    Whitespace    is       bad       .punctuated, text!!!!. corrrest text </p><a href='www.nlptest.com'>Go to link</a>\n",
      "\n",
      "Removing HTML Tags and URLs             : ThIs iS LowErCasE! special, characters! 123456 This is an example of removing stop words from a text document.    Whitespace    is       bad       .punctuated, text!!!!. corrrest text Go to link\n",
      "Lowercasing                             : this is lowercase! special, characters! 123456 this is an example of removing stop words from a text document.    whitespace    is       bad       .punctuated, text!!!!. corrrest text go to link\n",
      "Punctuation Removal                     : this is lowercase special characters 123456 this is an example of removing stop words from a text document    whitespace    is       bad       punctuated text corrrest text go to link\n",
      "Removing Special Characters and Numbers : this is lowercase special characters  this is an example of removing stop words from a text document    whitespace    is       bad       punctuated text corrrest text go to link\n",
      "Whitespace Removal                      : this is lowercase special characters this is an example of removing stop words from a text document whitespace is bad punctuated text corrrest text go to link\n",
      "Removing Stop Words                     : lowercase special characters example removing stop words text document whitespace bad punctuated text corrrest text go link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iscie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling Correction                     : lowercase special characters example removing stop words text document whitespace bad punctured text correct text go link\n",
      "\n",
      "After steps                             : lowercase special characters example removing stop words text document whitespace bad punctured text correct text go link\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Stop word list\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "text = \"<p>ThIs iS LowErCasE! special, characters! 123456 This is an example of removing stop words from a text document.    Whitespace    is       bad       .punctuated, text!!!!. corrrest text </p><a href='www.nlptest.com'>Go to link</a>\"\n",
    "print(f\"{'Before steps':<40}: {text}\")\n",
    "\n",
    "# Removing HTML Tags and URLs\n",
    "text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "print(f\"\\n{'Removing HTML Tags and URLs':<40}: {text}\")\n",
    "\n",
    "# Lowercasing\n",
    "text = text.lower()\n",
    "print(f\"{'Lowercasing':<40}: {text}\")\n",
    "\n",
    "# Punctuation Removal\n",
    "text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "print(f\"{'Punctuation Removal':<40}: {text}\")\n",
    "\n",
    "# Removing Special Characters and Numbers\n",
    "text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
    "print(f\"{'Removing Special Characters and Numbers':<40}: {text}\")\n",
    "\n",
    "# Whitespace Removal\n",
    "text = \" \".join(text.split())\n",
    "print(f\"{'Whitespace Removal':<40}: {text}\")\n",
    "\n",
    "# Removing Stop Words\n",
    "text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "print(f\"{'Removing Stop Words':<40}: {text}\")\n",
    "\n",
    "# Spelling Correction\n",
    "text = str(TextBlob(text).correct())\n",
    "print(f\"{'Spelling Correction':<40}: {text}\")\n",
    "\n",
    "print(f\"\\n{'After steps':<40}: {text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
