{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is the process of splitting a text into words, sentences, or meaningful units. In Natural Language Processing (NLP), raw text cannot be processed directly, so it is broken down into smaller parts to make it more meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is Tokenization Used?\n",
    "- To split text into meaningful units (e.g., words or sentences).\n",
    "- To enable machine learning and NLP models to process text more effectively.\n",
    "- To ensure preprocessing steps work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iscie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk  # Natural Language Tool Kit (nltk)\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text = \"Hello, World! How are you? Hi...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'World', '!', 'How', 'are', 'you', '?', 'Hi', '...']\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, World!', 'How are you?', 'Hi...']\n"
     ]
    }
   ],
   "source": [
    "# Sentence tokenization\n",
    "sentence_tokens = nltk.sent_tokenize(text)\n",
    "print(sentence_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
