{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNN)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of artificial neural networks specifically designed for processing sequential data. They are widely used in time-series forecasting, speech recognition, natural language processing (NLP), and more.\n",
    "\n",
    "![\"rnn\"](../images/4/4-rnn.png)\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "#### What is Time-Series and Sequential Data?\n",
    "\n",
    "Sequential data refers to any data where order matters. Examples include:\n",
    "\n",
    "- Time-Series Data: Ordered data points indexed by time (e.g., stock prices, weather data).\n",
    "- Natural Language Data: Words appear in a particular order in sentences.\n",
    "- Biological Sequences: DNA sequences follow a specific pattern.\n",
    "\n",
    "##### Examples of Sequential Data Applications\n",
    "\n",
    "- Speech Recognition: Converting spoken language into text.\n",
    "- Sentiment Classification: Analyzing text to determine emotional tone (positive/negative/neutral).\n",
    "- DNA Sequence Analysis: Identifying patterns in genetic sequences.\n",
    "  <br><br>\n",
    "\n",
    "---\n",
    "\n",
    "#### What is Sequential Dependency in Language and Time-Series Data?\n",
    "\n",
    "Sequential dependency refers to the relationship between data points across time. For instance:\n",
    "\n",
    "- In language, the meaning of a word depends on the context set by previous words.\n",
    "- In time-series data, the next value often depends on previous values (e.g., tomorrow's temperature depends on today’s and yesterday's temperatures).\n",
    "\n",
    "Mathematically, sequential dependency can be expressed as:\n",
    "\n",
    "$$\n",
    "P(x_t | x_{t-1}, x_{t-2}, ..., x_1)\n",
    "$$\n",
    "\n",
    "where each value \\( x_t \\) is dependent on previous values.\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Standard Neural Networks Fail for Sequential Data\n",
    "\n",
    "Standard feedforward neural networks (FNNs) process inputs independently and cannot capture dependencies between elements in a sequence. The main limitations are:\n",
    "\n",
    "1. Fixed Input Size: Cannot handle variable-length sequences.\n",
    "2. Lack of Memory: No mechanism to remember previous states.\n",
    "3. Inefficiency in Sequence Processing: Requires separate models for different sequence lengths.\n",
    "\n",
    "Due to these limitations, RNNs were introduced to effectively model sequential dependencies.\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "#### RNN Architecture and Working Mechanism\n",
    "\n",
    "- RNNs introduce the concept of hidden states to retain memory over time.\n",
    "\n",
    "##### Mathematical Representation:\n",
    "\n",
    "Given an input sequence \\( x_t \\), an RNN updates its hidden state \\( h_t \\) at each time step as follows:\n",
    "\n",
    "$$\n",
    " h_t = f(W \\cdot x_t + U \\cdot h_{t-1} + b)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( x_t \\) is the current input.\n",
    "- \\( h\\_{t-1} \\) is the previous hidden state.\n",
    "- \\( W, U, b \\) are learnable parameters.\n",
    "- \\( f \\) is an activation function (commonly tanh or ReLU).\n",
    "\n",
    "The final output can be computed as:\n",
    "\n",
    "$$\n",
    " y_t = g(V \\cdot h_t + c)\n",
    "$$\n",
    "\n",
    "where \\( V, c \\) are additional parameters and \\( g \\) is an activation function.\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "#### What is the Vanishing Gradient Problem?\n",
    "\n",
    "When training deep RNNs using backpropagation through time (BPTT), gradients may shrink exponentially, causing:\n",
    "\n",
    "- Loss of long-term dependencies: The model cannot learn dependencies across distant time steps.\n",
    "- Ineffective training: Weights stop updating due to near-zero gradients.\n",
    "\n",
    "Mathematically, if we compute the gradient of the loss \\( L \\) with respect to earlier states:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\prod_{t=1}^{T} W^T \\cdot \\frac{\\partial L}{\\partial h_T}\n",
    "$$\n",
    "\n",
    "If \\( W \\) has small eigenvalues, the gradients decay exponentially, leading to vanishing gradients.\n",
    "\n",
    "##### Solution: LSTM and GRU\n",
    "\n",
    "- To address this, architectures like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) introduce gating mechanisms to better retain long-term dependencies.\n",
    "  <br><br>\n",
    "\n",
    "---\n",
    "\n",
    "#### Applications of RNN in NLP\n",
    "\n",
    "RNNs are widely used in NLP for:\n",
    "\n",
    "##### 1. Machine Translation\n",
    "\n",
    "- Translating text from one language to another (e.g., English to French).\n",
    "\n",
    "##### 2. Sentiment Analysis\n",
    "\n",
    "- Classifying text into positive, negative, or neutral sentiments.\n",
    "\n",
    "##### 3. Named Entity Recognition (NER)\n",
    "\n",
    "- Identifying entities like names, locations, and dates in text.\n",
    "\n",
    "##### 4. Speech-to-Text Conversion\n",
    "\n",
    "- Recognizing spoken words and converting them into text.\n",
    "\n",
    "##### 5. Text Generation\n",
    "\n",
    "- Generating realistic and coherent text sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real-Life Application of RNN Using the IMDB Dataset\n",
    "\n",
    "- The dataset link &rarr; [IMDB_Dataset.csv](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/IMDB_Dataset.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iscie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "data[\"review\"] = [\n",
    "    \" \".join(\n",
    "        [\n",
    "            word.lower()\n",
    "            for word in sentence.translate(\n",
    "                str.maketrans(\"\", \"\", string.punctuation)\n",
    "            ).split()\n",
    "            if word.lower() not in stop_words\n",
    "        ]\n",
    "    )\n",
    "    for sentence in data[\"review\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 181543\n",
      "Max length: 1449\n",
      "X shape: (50000, 1449)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the review data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df[\"review\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"review\"])\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Vocab size:\", len(word_index))\n",
    "\n",
    "# Padding process\n",
    "maxlen = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=maxlen)\n",
    "print(\"Max length:\", maxlen)\n",
    "print(\"X shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y [1 1 1 ... 0 0 0]\n",
      "Y (50000,)\n"
     ]
    }
   ],
   "source": [
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[\"sentiment\"])\n",
    "print(\"Y\", y)\n",
    "print(\"Y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding: Word2Vec, Embedding matrix\n",
    "sentences = [text.split() for text in df[\"review\"]]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RNN Model\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=len(word_index) + 1,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=maxlen,\n",
    "        trainable=False,\n",
    "    )\n",
    ")\n",
    "model.add(SimpleRNN(units=100, return_sequences=False))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 183ms/step - accuracy: 0.6754 - loss: 0.5888 - val_accuracy: 0.6803 - val_loss: 0.5808\n",
      "Epoch 2/5\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 190ms/step - accuracy: 0.7011 - loss: 0.5611 - val_accuracy: 0.7116 - val_loss: 0.5516\n",
      "Epoch 3/5\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 183ms/step - accuracy: 0.7258 - loss: 0.5353 - val_accuracy: 0.7627 - val_loss: 0.4926\n",
      "Epoch 4/5\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 186ms/step - accuracy: 0.7599 - loss: 0.4950 - val_accuracy: 0.6300 - val_loss: 0.6542\n",
      "Epoch 5/5\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 183ms/step - accuracy: 0.7538 - loss: 0.5045 - val_accuracy: 0.7924 - val_loss: 0.4545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13341eb5b80>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train RNN Model\n",
    "model.fit(X_train, Y_train, epochs=5, batch_size=64, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 47ms/step - accuracy: 0.7935 - loss: 0.4556\n",
      "Test loss: 0.4545172154903412\n",
      "Test accuracy: 0.7924000024795532\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RNN Model\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification guessing\n",
    "def classify_sentence(sentence):\n",
    "    seq = tokenizer.texts_to_sequences([sentence])\n",
    "    padded_seq = pad_sequences(seq, maxlen=maxlen)\n",
    "\n",
    "    prediction = model.predict(padded_seq)\n",
    "    predicted_class = (prediction > 0.5).astype(int)\n",
    "    label = \"positive\" if predicted_class[0][0] == 1 else \"negative\"\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Guess: positive\n"
     ]
    }
   ],
   "source": [
    "pos_sentence = \"The movie was absolutely amazing! The storyline was captivating from start to finish, and the performances by the cast were outstanding. The visuals were stunning, and the soundtrack perfectly complemented the mood of each scene. It was an emotional rollercoaster, and I thoroughly enjoyed every moment of it. Highly recommend!\"\n",
    "pos_result = classify_sentence(pos_sentence)\n",
    "\n",
    "print(\"Guess:\", pos_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Guess: negative\n"
     ]
    }
   ],
   "source": [
    "neg_sentence = \"The movie was a huge disappointment. The plot was predictable and lacked depth, and the characters felt one-dimensional. The pacing was slow, making it hard to stay engaged, and the special effects were underwhelming. Overall, it didn’t live up to the hype, and I wouldn't recommend it.\"\n",
    "neg_result = classify_sentence(neg_sentence)\n",
    "\n",
    "print(\"Guess:\", neg_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
