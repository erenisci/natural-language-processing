{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "**BERT** is a transformer-based model developed by Google for language modeling. BERT is highly effective in understanding the meaning of language and achieves state-of-the-art results across a wide range of natural language processing tasks.\n",
    "\n",
    "### BERT Architecture\n",
    "\n",
    "- BERT is based on the **Transformer** architecture and processes text in a **bidirectional** manner. It considers both the left and right context of a word to understand its meaning.\n",
    "- The architecture uses a **self-attention** mechanism, which allows the model to learn the relationships between words and their context in the text. This helps in better capturing the contextual information in the language.\n",
    "- BERT consists of only the **Encoder** part of the Transformer and does not use the **Decoder**. Therefore, it is designed for understanding text representations rather than generating text.\n",
    "- BERT is trained using two main tasks: **Masked Language Modeling (MLM)** and **Next Sentence Prediction (NSP)**.\n",
    "\n",
    "![\"bert\"](../images/4/4-bert.png)\n",
    "\n",
    "### BERT Features\n",
    "\n",
    "- **Bidirectional Context**: BERT learns the meaning of each word by considering the context from both directions (left and right). This makes it highly effective at understanding context.\n",
    "- **Masked Language Modeling (MLM)**: BERT randomly masks some words in the input text and asks the model to predict these masked words based on their surrounding context. This allows the model to learn context-dependent word representations.\n",
    "- **Next Sentence Prediction (NSP)**: BERT is trained to predict whether two given sentences are consecutive in a text, which helps it understand the relationship between sentences and improve tasks like question answering and sentence classification.\n",
    "\n",
    "BERT is widely used for various NLP tasks such as text classification, named entity recognition (NER), and question answering, among others.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
