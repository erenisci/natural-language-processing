{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference Between Transformer-Based Models\n",
    "\n",
    "| Feature / Model          | **BERT**                                                                | **GPT**                                                 | **LLaMA**                                                                |\n",
    "| ------------------------ | ----------------------------------------------------------------------- | ------------------------------------------------------- | ------------------------------------------------------------------------ |\n",
    "| **Architecture**         | Bidirectional (Encoder)                                                 | Unidirectional (Decoder)                                | Decoder-only (Similar to GPT)                                            |\n",
    "| **Training Objective**   | Masked Language Modeling (MLM), Next Sentence Prediction (NSP)          | Autoregressive (Predicting the next word in a sequence) | Efficient training with large datasets (Autoregressive)                  |\n",
    "| **Context Processing**   | Bidirectional (Both left and right context)                             | Unidirectional (Left-to-right)                          | Autoregressive, but more efficient                                       |\n",
    "| **Key Task Types**       | Text classification, Named Entity Recognition (NER), Question Answering | Text generation, Dialogue systems, Content creation     | Text generation, Question answering, Summarization                       |\n",
    "| **Primary Strength**     | Understanding context from both directions                              | Generating fluent and coherent text                     | Efficient large-scale language modeling                                  |\n",
    "| **Pretraining Strategy** | MLM (Masked tokens) and NSP (Next sentence prediction)                  | Autoregressive (Predict next token)                     | Large-scale pretraining with an emphasis on efficiency                   |\n",
    "| **Applications**         | Tasks that require understanding context (Classification, QA, NER)      | Content generation, Text completion, Dialogue systems   | Efficient language modeling tasks (e.g., text generation, summarization) |\n",
    "| **Performance Focus**    | Contextual understanding and bidirectional learning                     | Text generation fluency and coherence                   | Efficiency in scaling large models with fewer resources                  |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
