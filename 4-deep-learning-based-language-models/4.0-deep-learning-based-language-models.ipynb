{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning-Based Language Models\n",
    "\n",
    "Deep learning-based modern language models can learn much larger datasets and capture more complex language structures compared to traditional statistical and probabilistic models. These models have revolutionized natural language processing (NLP) applications.\n",
    "\n",
    "![\"deep-learning-based-language-models\"](../images/4/4-deep-learning-based-language-models.png)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Word Embeddings\n",
    "\n",
    "Word embeddings are techniques that transform words into fixed-dimensional dense vectors. These vectors mathematically represent the semantic similarities and relationships between words.\n",
    "\n",
    "##### Important Word Embedding Models\n",
    "\n",
    "- Word2Vec (CBOW & Skip-gram)\n",
    "- GloVe (Global Vectors)\n",
    "- FastText (Uses subword information)\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Captures semantic relationships between words.\n",
    "- Can learn phrase structures and contextual relationships.\n",
    "- Efficient and fast, forming the foundation of many NLP models.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Produces fixed-length vectors, which cannot fully model context.\n",
    "- Struggles with homonymy (same spelling, different meanings) and polysemy (words with multiple meanings).\n",
    "- Learned vectors are static, meaning they cannot differentiate word meanings based on context.\n",
    "  <br>\n",
    "  <br>\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Recurrent Neural Networks (RNN)\n",
    "\n",
    "RNNs are neural networks designed to process sequential data. They are widely used in NLP and time-series forecasting.\n",
    "\n",
    "##### How it Works\n",
    "\n",
    "- Stores previous time-step information to influence future predictions.\n",
    "- Uses a hidden state vector to model past information.\n",
    "\n",
    "$$\n",
    "h_t = f(W \\cdot x_t + U \\cdot h_{t-1} + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( x_t \\) → Current input\n",
    "- \\( h\\_{t-1} \\) → Previous hidden state\n",
    "- \\( W, U, b \\) → Learnable parameters\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Suitable for sequential data.\n",
    "- Can learn context and determine a word’s meaning based on its position in a sentence.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- **Vanishing Gradient Problem** makes modeling long dependencies difficult.\n",
    "- Poor parallelization since computations must be done sequentially.\n",
    "  <br>\n",
    "  <br>\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Long Short-Term Memory (LSTM)\n",
    "\n",
    "LSTM is an improved version of RNNs and is much better at modeling long-term dependencies.\n",
    "\n",
    "##### How it Works\n",
    "\n",
    "- Uses an **input gate**, **forget gate**, and **output gate** to control information flow.\n",
    "- The memory cell forgets irrelevant information and retains important data.\n",
    "\n",
    "$$\n",
    "c_t = f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c}_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( c_t \\) → Memory cell\n",
    "- \\( f_t \\) → Forget gate\n",
    "- \\( i_t \\) → Input gate\n",
    "- \\( \\tilde{c}\\_t \\) → New memory state\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Can learn long-term dependencies.\n",
    "- Solves the **Vanishing Gradient Problem**.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- High computational cost.\n",
    "- Poor parallelization since previous steps must be computed first.\n",
    "  <br>\n",
    "  <br>\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Transformer Models\n",
    "\n",
    "The Transformer architecture has revolutionized NLP and is the foundation of modern large language models. It is much more efficient and powerful due to **parallelized computation**.\n",
    "\n",
    "##### Self-Attention Mechanism\n",
    "\n",
    "- Computes relationships between all words in a sentence.\n",
    "- Does not rely only on previous words but sees the entire context.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Excellent at capturing long-term dependencies.\n",
    "- Highly parallelizable.\n",
    "- Performs better on large datasets.\n",
    "  <br>\n",
    "  <br>\n",
    "\n",
    "##### **Important Transformer Models:**\n",
    "\n",
    "#### **4.1** BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "- Uses **bidirectional attention**, meaning it processes all words in a sentence at once.\n",
    "- Learns from large datasets using a **pretraining strategy**.\n",
    "\n",
    "##### Use Cases:\n",
    "\n",
    "- Text classification\n",
    "- Named Entity Recognition (NER)\n",
    "- Question answering systems\n",
    "  <br>\n",
    "  <br>\n",
    "\n",
    "#### **4.2** GPT (Generative Pretrained Transformer\n",
    "\n",
    "- Uses **unidirectional** modeling (predicts words based on previous words).\n",
    "- Great for **text generation**.\n",
    "- Forms the basis of **large-scale language models (LLM)**.\n",
    "\n",
    "##### Use Cases:\n",
    "\n",
    "- Chatbots\n",
    "- Text generation\n",
    "- Summarization\n",
    "  <br>\n",
    "  <br>\n",
    "\n",
    "#### **4.3** LLaMA (Large Language Model Meta AI\n",
    "\n",
    "- Developed by **Meta (Facebook)**.\n",
    "- Similar to GPT but optimized for better performance with **less data**.\n",
    "- Popular in the **open-source** community.\n",
    "\n",
    "##### Use Cases:\n",
    "\n",
    "- Research and Academic Studies\n",
    "- Custom LLM Applications\n",
    "- Lightweight Language Models\n",
    "- Open-Source Development\n",
    "  <br>\n",
    "  <br>\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparative Summary Table**\n",
    "\n",
    "| Model           | Long Dependency Learning | Parallelization | Special Use Case          |\n",
    "| --------------- | ------------------------ | --------------- | ------------------------- |\n",
    "| Word Embeddings | ❌ No                    | ✅ Yes          | Word semantic relations   |\n",
    "| RNN             | ❌ Weak                  | ❌ No           | Sequence-based processing |\n",
    "| LSTM            | ✅ Yes                   | ❌ No           | Long dependencies         |\n",
    "| Transformer     | ✅ Excellent             | ✅ Yes          | NLP, LLMs                 |\n",
    "| BERT            | ✅ Bidirectional         | ✅ Yes          | NLP understanding         |\n",
    "| GPT             | ✅ Unidirectional        | ✅ Yes          | Text generation           |\n",
    "| LLaMA           | ✅ Optimized             | ✅ Yes          | Lightweight LLM           |\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "- **Word Embeddings** transform words into vectors, capturing semantic relationships.\n",
    "- **RNN & LSTM** are good for sequential data, with LSTM being better at long dependencies.\n",
    "- **Transformer models (BERT, GPT, LLaMA)** provide the best performance in modern NLP and form the foundation of **large-scale language models**.\n",
    "\n",
    "The future is completely shaped by **Transformer-based models**!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
