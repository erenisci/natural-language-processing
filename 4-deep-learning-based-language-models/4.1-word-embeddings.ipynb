{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "\n",
    "- ##### Why Should We Represent Words as Vectors?\n",
    "\n",
    "  - Traditional one-hot encoding does not capture semantic relationships.\n",
    "  - Word embeddings map words into dense vector spaces, preserving meaning and context.\n",
    "  - Helps NLP models understand similarities and analogies between words.\n",
    "  - Enables more effective machine learning models by reducing dimensionality.\n",
    "    <br>\n",
    "    <br>\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Word2Vec\n",
    "\n",
    "##### What is Word2Vec?\n",
    "\n",
    "- Word2Vec is a neural network-based model that learns word representations in a continuous vector space, allowing similar words to have closer representations.\n",
    "\n",
    "##### Key Features and Disadvantages\n",
    "\n",
    "- Features:\n",
    "\n",
    "  - Learns vector representations based on context.\n",
    "  - Can capture word analogies (e.g., `king - man + woman ≈ queen`).\n",
    "  - Efficient and scalable for large corpora.\n",
    "\n",
    "- Disadvantages:\n",
    "\n",
    "  - Produces static word embeddings (the same vector for a word in different contexts).\n",
    "  - Cannot handle out-of-vocabulary words.\n",
    "  - Ignores subword information.\n",
    "\n",
    "#### Who Developed It?\n",
    "\n",
    "- Developed by Tomas Mikolov and his team at Google in 2013.\n",
    "\n",
    "#### Dataset Used\n",
    "\n",
    "- Trained on large corpora such as Google News dataset (100B words).\n",
    "\n",
    "#### Core Models\n",
    "\n",
    "##### Continuous Bag of Words (CBOW)\n",
    "\n",
    "- Predicts a word based on its surrounding context.\n",
    "- Efficient and faster for training.\n",
    "\n",
    "##### Skip-gram\n",
    "\n",
    "- Predicts surrounding words given a central word.\n",
    "- Works better for learning rare words.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. FastText\n",
    "\n",
    "#### What is FastText?\n",
    "\n",
    "- FastText is an extension of Word2Vec that incorporates subword information to generate better word representations.\n",
    "\n",
    "#### Key Features and Disadvantages\n",
    "\n",
    "- Features:\n",
    "\n",
    "  - Uses subword embeddings, allowing it to handle out-of-vocabulary (OOV) words.\n",
    "  - More effective for morphologically rich languages.\n",
    "  - Retains advantages of Word2Vec while improving rare word representation.\n",
    "\n",
    "- Disadvantages:\n",
    "\n",
    "  - Increased computational complexity due to subword modeling.\n",
    "  - Requires more storage compared to standard Word2Vec.\n",
    "\n",
    "#### Who Developed It?\n",
    "\n",
    "- Developed by Facebook AI Research (FAIR) in 2016.\n",
    "\n",
    "#### Dataset Used\n",
    "\n",
    "- Typically trained on Wikipedia and Common Crawl datasets.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Word2Vec vs FastText Comparison\n",
    "\n",
    "| Feature               | Word2Vec       | FastText   |\n",
    "| --------------------- | -------------- | ---------- |\n",
    "| Subword Handling      | ❌ No          | ✅ Yes     |\n",
    "| OOV Words             | ❌ Not Handled | ✅ Handled |\n",
    "| Computational Cost    | ✅ Lower       | ❌ Higher  |\n",
    "| Rare Word Performance | ❌ Weak        | ✅ Strong  |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Vector Space and Semantic Similarity\n",
    "\n",
    "- Vector Space Representation: Each word is mapped to a high-dimensional space, where similar words appear closer to each other.\n",
    "- Semantic Similarity: Words with similar meanings have higher cosine similarity in the vector space.\n",
    "- Word Analogies: Relationships like _Paris - France + Italy ≈ Rome_ can be learned through vector operations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
