{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification (w/ Machine Learning)\n",
    "\n",
    "Text Classification is the task of categorizing text into predefined categories or classes. It is typically done using supervised learning techniques, where the model predicts the category of a given text based on its content.\n",
    "\n",
    "- Common examples of text classification include:\n",
    "\n",
    "  - Spam Detection: Classifying emails as \"spam\" or \"ham\" (non-spam).\n",
    "  - Sentiment Analysis: Determining whether a piece of text expresses positive, negative, or neutral sentiment.\n",
    "  - Topic Categorization: Classifying articles into topics such as sports, politics, technology, etc.\n",
    "\n",
    "In short, text classification involves analyzing the content of text and assigning it to one or more predefined categories.\n",
    "\n",
    "#### Text Classification Examples:\n",
    "\n",
    "- **Spam Detection**: Email -> \"spam\" or \"ham\"\n",
    "- **Sentiment Analysis**: Text -> \"positive\", \"negative\", or \"neutral\"\n",
    "- **Topic Categorization**: News article -> \"Sports\", \"Politics\", \"Technology\"\n",
    "\n",
    "#### Text Classification Flowchart:\n",
    "\n",
    "![\"text-classification\"](../images/5/5-text-classification-flowchart.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real-Life Application of Text Classification Using the Spam Dataset &rarr; [Spam_Dataset.csv](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iscie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\iscie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\iscie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libs\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "# The dataset\n",
    "data = pd.read_csv(\"../data/Spam_Dataset.csv\", encoding=\"latin-1\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1, inplace=True)\n",
    "\n",
    "data.columns = [\"label\", \"text\"]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    0\n",
      "text     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# EDA: Missing values\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "\"\"\"\n",
    "    Special Chars\n",
    "    Lowercase\n",
    "    Tokenize\n",
    "    Remove Stopwords\n",
    "    Lemmatize\n",
    "\"\"\"\n",
    "\n",
    "text = list(data[\"text\"])\n",
    "\n",
    "corpus = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(text)):\n",
    "    r = re.sub(r\"^a-zA-Z\", \" \", text[i])\n",
    "    r = r.lower()\n",
    "    r = r.split()\n",
    "    r = [word for word in r if word not in stopwords.words(\"english\")]\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "    r = \" \".join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "\n",
    "data[\"text2\"] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split (67% - 33%)\n",
    "X = data[\"text2\"]\n",
    "Y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.33, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction: BoW\n",
    "cv = CountVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier training: Model training\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train_cv, Y_train)\n",
    "\n",
    "x_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "predictions = dt.predict(x_test_cv)\n",
    "\n",
    "c_matrix = confusion_matrix(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.6073953235454\n"
     ]
    }
   ],
   "source": [
    "# Acc\n",
    "print(\n",
    "    \"Accuracy:\",\n",
    "    100 * (sum(sum(c_matrix)) - c_matrix[1, 0] - c_matrix[0, 1]) / sum(sum(c_matrix)),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
