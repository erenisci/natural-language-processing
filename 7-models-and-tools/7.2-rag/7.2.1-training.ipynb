{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG (Retrieval-Augmented Generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel, BertTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Corpus (Database) - Documents\n",
    "documents = [\n",
    "    \"BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model for natural language understanding. It works by pre-training a language model on a large corpus of text and fine-tuning it on specific tasks like question answering and sentiment analysis.\",\n",
    "    \"Machine learning is a subset of artificial intelligence that involves algorithms learning from data to make predictions. Supervised learning, unsupervised learning, and reinforcement learning are the main types of machine learning.\",\n",
    "    \"Natural language processing (NLP) is a field of AI focused on the interaction between computers and human language, enabling tasks like text classification, translation, and sentiment analysis.\",\n",
    "    \"GPT (Generative Pretrained Transformer) is a large language model based on the Transformer architecture. It is capable of generating coherent and contextually relevant text, used for a variety of tasks like text completion and dialogue generation.\",\n",
    "    \"Deep learning is a subset of machine learning that uses neural networks with many layers (also known as deep neural networks) to model complex patterns in large amounts of data.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is BERT?\n",
      "Most Similar Document: GPT (Generative Pretrained Transformer) is a large language model based on the Transformer architecture. It is capable of generating coherent and contextually relevant text, used for a variety of tasks like text completion and dialogue generation.\n"
     ]
    }
   ],
   "source": [
    "# 1. Information Retrieval with BERT\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Get the user query\n",
    "query = \"What is BERT?\"\n",
    "\n",
    "# Encode the user query\n",
    "inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Compute the embeddings for the documents\n",
    "document_embeddings = []\n",
    "for doc in documents:\n",
    "    inputs = tokenizer(doc, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        doc_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        document_embeddings.append(doc_embedding)\n",
    "\n",
    "# Compute cosine similarity between the query and each document\n",
    "similarities = cosine_similarity([query_embedding], document_embeddings)\n",
    "\n",
    "# Select the most similar document\n",
    "most_similar_doc_index = np.argmax(similarities)\n",
    "most_similar_doc = documents[most_similar_doc_index]\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Most Similar Document: {most_similar_doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is BERT?\n",
      "\n",
      "Context: GPT (Generative Pretrained Transformer) is a large language model based on the Transformer architecture. It is capable of generating coherent and contextually relevant text, used for a variety of tasks like text completion and dialogue generation.\n",
      "\n",
      "Answer: Bert is an open source language that is designed to be used in a wide variety, from text processing to text editing. BPT is based upon the B.P.T. (Bert-Tutorial) framework, which is used to build and maintain Bpt. The BTS is also a framework for building and maintaining BNTs. In this article, we will look at the various BTTs that are available for use in BBT. We will also look into the different BTs available in the following languages:\n",
      ".NET Core,.NET Framework 4.5, and.Net Framework 5.0.1. This article will focus on BFTs, BCTs and BDTs in general. These languages are not included in this list because they are only available as part of the project. They are also not part or in any way related to BMT. For more information about BTM, see the official Btt wiki.\n"
     ]
    }
   ],
   "source": [
    "# 2. Text Generation with GPT-2\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Prepare the input for GPT-2: Use the query and the most similar document\n",
    "input_text = f\"Question: {query}\\n\\nContext: {most_similar_doc}\\n\\nAnswer:\"\n",
    "\n",
    "# Encode the input for GPT-2\n",
    "input_ids = gpt2_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Create attention mask (since it's a single input, this is just 1 for all tokens)\n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "# Generate the answer\n",
    "with torch.no_grad():\n",
    "    generated_ids = gpt2_model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=512,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=gpt2_tokenizer.eos_token_id,  # Set pad_token_id to eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode the generated answer\n",
    "generated_text = gpt2_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
